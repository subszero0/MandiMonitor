# Testing Quick Reference Card

> **Keep this handy during development**

## ğŸš¦ **The Five Testing Levels**

1. **Unit Tests** âœ… *Test functions in isolation*
2. **Integration Tests** âœ… *Test components together*  
3. **Simulation Tests** ğŸ†• *Test real-world scenarios*
4. **Manager Demo Tests** ğŸ¯ *Zero-tolerance validation*
5. **Manual Testing** ğŸš¨ *Reality validation (CRITICAL)*

## âš¡ **Quick Checklist for Any Feature**

```
â–¡ Unit test with clean data
â–¡ Integration test with realistic data
â–¡ Test with broken/missing data
â–¡ Test async context compatibility
â–¡ Test failure scenarios
â–¡ Test concurrent access
â–¡ Manager demo scenario
â–¡ Manual testing with production data patterns
```

## ğŸ”¥ **Critical Questions to Ask**

- What happens if the API is down?
- What if the database is full?
- What if users input malicious data?
- What if everything fails at once?
- Would I be embarrassed to demo this?

## ğŸ¯ **Success Metrics**

- âœ… **95%+ success rate** under load
- âœ… **Sub-1000ms response times**
- âœ… **Zero crashes** during failures
- âœ… **Graceful error messages**

## ğŸš¨ **Red Flags**

- âŒ "It works on my machine"
- âŒ Only testing happy paths
- âŒ Perfect test data only
- âŒ No failure scenario tests
- âŒ Performance as afterthought

## ğŸ’ **Golden Rules**

1. **"If it can fail in production, it should fail in testing first"**
2. **"Test with data that looks like production, not like your dreams"**
3. **"Perfect mocks create dangerous false confidence"**
4. **"Manual testing reveals what automated testing misses"**
5. **"If you'd be embarrassed to show this to your boss, fix it"**

## ğŸ› ï¸ **Common Test Patterns**

### **Type Safety Test**
```python
def test_handles_mixed_types():
    products = [
        {"price": 50000},      # int
        {"price": "N/A"},      # string
        {"price": None}        # None
    ]
    result = filter_products(products, 60000)
    assert result is not None  # Should not crash
```

### **Failure Recovery Test**
```python
async def test_api_failure_recovery():
    with mock_api_failure():
        result = await search_products("test")
        # Should return graceful fallback, not crash
        assert result is not None
```

### **Concurrent Access Test**
```python
async def test_concurrent_users():
    tasks = [simulate_user(i) for i in range(50)]
    results = await asyncio.gather(*tasks)
    failures = [r for r in results if "FAILED" in str(r)]
    assert len(failures) == 0
```

---
**Keep this card visible while coding!**
